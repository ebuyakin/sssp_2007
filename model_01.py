# MODEL OF SENTENCE PRODUCTION
"""
implementation of the model
"""

import importlib
import numpy as np
import pandas as pd
from types import SimpleNamespace
from matplotlib import pyplot as plt
from anytree import Node, RenderTree
import language_01
importlib.reload(language_01)


class Model:

    def __init__(self):

        # PARAMETERS OF THE MODEL:
        # ====================================================================================================

        # 1. Language parameters:
        self.num_concepts = 30  # size of the concepts inventory
        self.num_words = 50  # size of the lexicon

        # 1.1. parameters for the external ML-1 language generator.
        self.is_derivative_dictionary = False  # whether to create random or derivative dictionary
        self.proximity_setting = [0, 0, 1, 0, 2]  # used in generation of derivative dictionary

        # 2.0 Random message generation parameters:
        self.branching_probs = [[0.0, 0.2, 0.5, 0.3],
                                [0.2, 0.3, 0.5],
                                [0.7, 0, 0.3]]
        # probs set the distribution of the number of children at different levels of the message tree.
        # eg. self.probs[0][1] = 0.2 means that at the top level there is a 0.2 chance to have 1 child,
        # self.probs[2][3] = 0 means at level 2 (this is grand_children level considering top level = 0)
        # there is 0 chance of having 3 children (but it can be larger or smaller number of children).
        # the list can have any number sub-lists with any number of elements. the sum of elements in the
        # sub-list shall be 1.

        # 2.1 Structured message generation parameters:
        self.message_structure = ['x', 'x0', 'x1', 'x01', 'x02']  # pre-defined structure of the message tree
        # to create a message of a pre-defined structure the list of node_codes is used. The node_codes
        # define the relationship between the nodes in a desired message. E.g. ['x', 'x0', 'x1', 'x01', 'x02']
        # is the message the consists of a top node (x), two children (x0, x1) and two grandchildren from
        # the first child (x01, x02). The message is generated by the method generate_structured_message

        # 3. Message encoding parameters:
        self.max_number_sub_tree_steps = 100  # maximum number of sub-trees to be tried in message encoding
        # this is the upper limit (hardly can be exceeded) set up to avoid infinite cycle in case of errors.

        # =====================================================================================================

        # 4. Sub_tree selection parameters:
        self.sub_tree_depth_probs = [0, 0, 0, 1]  # probability distribution for the depth of the sub_tree.
        # e.g. self.sub_tree_depth_probs[1] = 0.1 means that there is a 0.1 chance that the max depth of the
        # sub-tree will be 1 level.
        self.sub_tree_growth_rate = 1  # probability of adding the node to the sub-tree. the selection of the
        # sub_tree is made by crawling through the allowed levels of the message tree and deciding randomly
        # to choose the node or not.

        # 5. Sub-tree lexicalization:
        self.distance_threshold_multiplier = 1  # the maximum distance for the lexicalization to be accepted
        # the distance between the sub-tree and corresponding sentence.
        self.increment_threshold = 0.3  # how much a given word shall decrease the distance to be included
        # in the sentence.

        # 5.1. Selection of the sub-tree head:
        self.level_preference_multiplier = [3, 3, 2, 1]  # in choosing the top of the expanded tree these multipliers
        # are applied. self.level_preference_multiplier[0]=3 means thee is a 3 multiplier for the probability
        # of selecting the as the head concept for the sub_tree the node at the level 0.

        # 6. Selection of the node for expansion:
        self.sentence_tree_shape = [5, 4, 3, 3, 3, 2, 2, 1]  # the constraint of the sentence growth
        # sets the maximum number of children per node at each level of the lexical tree.
        # e.g. sentence_tree_shape[2] = 3 means that at the 2nd level of the tree each node shall have
        # maximum 3 children. this parameter is utilized by select_node_for_expansion
        self.level_expansion_multiplier = [2, 1, 0.5]  # preference to expand higher levels
        self.n_children_expansion_multiplier = [3, 2, 1]  # preference to expand nodes with less children

        # 7. Lexicalization of a concept:
        self.lexicalization_noise_level = 0.5  # the weight of the random component

        # 8. Syntactic search:
        self.collocations_noise_level = 0.5  # noise level in search of syntactic associations

        # 9. Distance measurement:
        self.concept_level_weights = [1, 1, 1, 1, 1]  # weight in distance measurement of each level
        # of the concept tree
        self.lexical_tree_weights = [1, 1, 1, 1, 1]  # weights of the levels of the lexical tree

        # 10. Duration of the action:
        self.duration_sub_tree_construction = 0.2  # how long does it take to build a sub-tree
        self.duration_head_word_search = 0.05  # how long does semantic search take
        self.duration_root_search = 0.01  # how long does root selection take
        self.duration_leaf_search = 0.01  # how long does the leaf search take

        # SEARCH LIMITS
        self.head_word_max_attempts = 10
        self.expansion_root_node_max_attempts = 10
        self.expansion_leaf_node_max_attempts = 10

    # =============================================================================================================
    # language construction/import methods:
    # =============================================================================================================


    def create_basic_language(self):
        """
        creates basic random language. the semantic memory (inventory of concepts) and lexicon (inventory of words,
        dictionary) are just lists of integer numbers (indices). they have no labels or any real-world referents.
        semantics - associations between concepts and words generated randomly using zipf-distribution to simulate
        the situation when each concept has a small number of strong associates (good lexicalizations) and a large
        number of weak associates (poor, but sometimes acceptable lexicalizations).
        syntax - associations between the words (collocations, language model) is generated randomly too. also based
        on zipf distribution to reflect the fact that each word has a small number of close collocates and a large
        number of remote collocates.
        in both cases parameters of the zipf distribution vary from concept to concept and from word to word.
        :return:
        """

        self.concepts_inventory = list(range(self.num_concepts))  # concept inventory is a list indices [1, 2, ...]
        self.lexicon = list(range(self.num_words))  # lexicon is similarly just a list of indices

        # create semantic associations matrix
        self.semantics = np.zeros((self.num_words,self.num_concepts))
        for i in range(self.num_concepts):
            lex_diversity_ratio = np.random.uniform(1.2, 2)  # coefficient for zipf distribution
            zipf_sample = np.array([1/lex_diversity_ratio**i for i in range(self.num_words)])  #
            zipf_sample = np.random.permutation(zipf_sample)
            self.semantics[:, i] = zipf_sample

        # create syntactic association matrix
        self.collocations = np.zeros((self.num_words, self.num_words))
        for i in range(self.num_words):
            syntactic_diversity_ratio = np.random.uniform(1.2, 2)  # coefficient for zipf distribution
            zipf_sample = np.array([1/syntactic_diversity_ratio**i for i in range(self.num_words)])  #
            zipf_sample = np.random.permutation(zipf_sample)
            self.collocations[:, i] = zipf_sample


    def create_language_from_ML1(self):
        """
        generates language from language_01 module.
        :return: self.concepts_inventory, self.lexicon, self.semantics, self.collocations
        """

        from language_01 import ModelLanguage
        self.language = ModelLanguage()
        self.language.generate_language(concept_store_size=self.num_concepts,
                                        dictionary_size=self.num_words,
                                        is_connected=self.is_derivative_dictionary,
                                        proximity_setting=self.proximity_setting)

        self.concepts_inventory = self.language.concepts  # the inventory of concepts, semantic memory
        self.lexicon = self.language.dictionary  # lexicon, the inventory of words
        self.semantics = self.language.semantics  # semantic associations matrix
        self.collocations = self.language.collocations  # syntactic associations (language model, collocations)


    def generate_random_message(self):
        """
        the method generates the tree of concepts in accordance with branching probabilities set in self.probs
        the nodes of the message tree is encoded as a flat list with nodes as elements of the list
        the nodes have a format (node_code, node_value), where node_code is a string
        encoding the position of the nod in the tree. the length of the node_code corresponds to the level
        of the node in the tree and the numbers show the parents indices. Thus, the connections between
        the nodes are encoded in node_code
        e.g. 'n114' is the 3rd level concept in the message tree and it is in the branch 1.1.4 of the tree
        n114 is a 4th child of the node n11, n11 is the first child of the n1, and n1 is the first child of
        the top node of the message.
        method also adds activation of each node that is used in prioritizing lexicalization
        :return:  self.message - the list of nodes
        """

        def branching(node_code, node_value, node_label, branching_probs):  # recursive function building the tree
            activation = np.float16(np.random.normal(0, 1))  # random activation value of the current node
            parent_connection = np.float16(np.random.normal(0,1))  # random strength of association with the parent

            tree = [{'code': node_code,
                     'value': node_value,
                     'label': node_label,
                     'activation': activation,
                     'parent_connection': parent_connection,
                     'is_lexicalized': False,
                     'is_selected': False}]  # current node assignment

            tree_level = len(node_code) - 1  # check how many tiers in the tree
            if tree_level < len(branching_probs):
                children_range = range(len(branching_probs[tree_level]))
                num_children = np.random.choice(children_range, p=branching_probs[tree_level])
                children = np.random.randint(0, self.num_concepts, num_children)
                for i, child in enumerate(children):
                    node_code_ = node_code + str(i)
                    tree_ = branching(node_code_, child, self.concepts_inventory[child], branching_probs)
                    tree += tree_
            return tree

        top_concept = np.random.randint(0, self.num_concepts)  # choose the top concept (index of the top concept)
        self.message = branching('x',top_concept, self.concepts_inventory[top_concept], self.branching_probs)


    def generate_structured_message(self):
        """
        method for generating a random message according to specified structure.
        the structure is given as a list of node codes.
        """

        self.message = []
        used_concepts = []
        for node_code in self.message_structure:
            activation = np.float16(np.random.normal(0, 1))  # random activation value of the current node
            parent_connection = np.float16(np.random.normal(0,1))  # random strength of association with the parent
            node_value = np.random.choice(range(self.num_concepts))  # choose random

            while node_value in used_concepts:  # if it's used already
                node_value = np.random.choice(range(self.num_concepts))  # try again

            used_concepts.append(node_value)  # memorize the used concept
            node_label = self.concepts_inventory[node_value]
            node = {'code': node_code,
                    'value': node_value,
                    'label': node_label,
                    'activation': activation,
                    'parent_connection': parent_connection,
                    'is_lexicalized': False,
                    'is_selected': False}
            self.message.append(node)


    # =============================================================================================================
    # essential lexicalization methods:
    # =============================================================================================================


    def lexicalize_message(self):
        """
        method looks for a linguistic expression of the message. one message (or description of a scene)
        can be encoded as one or several sentences (text, language)
        sentence is a list of dictionaries (each dictionary is a node of the syntactic tree)
        node =  {'code': 's0001', 'value': 3}
        this is the top-level method that implements that loop over sub-trees of the message.
        the message (the tree of concept) is not lexicalized in one go (as it might be too big for one iteration),
        so it's broken to sub-trees (method select_sub_tree). the sub-tree is selected by marking the elements
        of the message ('is_selected' = True). The method lexicalize than takes the selected sub-tree, produces
        sentence (one sub-tree generates strictly one sentence) and adds it to the sentence_list.

        the record of the timeline of the lexicalization is stored in dataframe self.log_encoding.
        the dataframe contains 5 types of records:
        1. new message sub_tree is selected: programmed in method lexicalize_message()
        2. new head word for lexicalization is selected: programmed in method lexicalize_sub_tree()
        3. new expansion root is selected: programmed in method get_best_sentence_for_head_word()
        4. new expansion leaf is selected: programmed in method get_best_expansion_leaf_tree()

        :return: self.sentence_set() is the main output of the lexicalize_method(). it's a list of lists
        where the elements of the outer lists are sentences. Each sentence is a list of dictionaries
        with each dictionary corresponding to one node of the sentence.
        """

        # log dataframe to collect the lexicalization process statistical data for every step:
        self.log_dict = {'time': '',  # time of the event
                         'record_type': '',  # what event given record signals
                         'sub_tree': '',  # condensed (only labels) version of the current message sub-tree
                         'threshold': '', # the distance threshold for lexicalization to be accepted
                         'head_word': '',  # full node dictionary for the head word of the current lexical tree
                         'semantic_strength': '',  # distance between the head word and top concept
                         'tested_head_words': '',  # the words that have been previously tested (indices from lexicon)
                         'current_sentence': '',  # the state of the lexical tree at the moment of recorded event.
                         'candidate_sentence': '',  # candidate expansion sentence
                         'root_code': '',  # code of the expansion_root node
                         'excluded_root_codes': '', # expansion roots that have been tried already
                         'root_word': '',  # the word label that occupies the expansion root node
                         'root_counter': '',  # number of expansion roots tested for a given head word
                         'leaf_word': '',  # the word label for the tested leaf
                         'leaf_counter': '',  # number of leafs tested
                         'syntactic_strength': '',  # association between the root and the leaf
                         'distance': '',  # the semantic distance between lexical tree and the message sub-tree
                         'candidate_distance': '',  # the same for the candidate for expansion tree
                         'verdict': ''}  # verdict is the decision made regarding the lexical tree
        log_cols = list(self.log_dict.keys())  # list of columns for the log dataframe
        self.log_encoding = pd.DataFrame(columns=log_cols)

        self.sentence_set = []  # initialize text. this is a list of sentences to store linguistic expression
        self.counter_sub_trees = 0  # counter of the number of cycles
        self.timing = 0  # chronometer of lexicalization
        lexicalization_complete = False  # set the flag for completion of lexicalization

        while not lexicalization_complete:  # cycle of lexicalization of the message fragments

            self.select_sub_tree_for_lexicalization()  # mark subtree for lexicalization
            self.timing += self.duration_sub_tree_construction  # update time
            self.condensed_sub_tree = [n['label'] for n in self.message if n['is_selected']]  # concise rep
            n_selected_nodes = sum([1 for x in self.message if x['is_selected']])  # number of selected nodes

            # max distance for the sentence to be accepted as lexicalization of the sub-tree:
            self.distance_threshold = len(self.condensed_sub_tree) * self.distance_threshold_multiplier

            # -------------------------------------------------------------------------------------------------
            # make a log record:
            rec = {'record_type': 'new_sub_tree', 'sub_tree': self.condensed_sub_tree,
                   'threshold': str('%.1f' % self.distance_threshold), 'time': float('%.2f' % self.timing)}
            rec_ = {k:self.log_dict[k] if k not in rec else rec[k] for k in self.log_dict}  # to replace nans
            self.log_encoding = self.log_encoding.append(rec_, ignore_index=True)
            # -------------------------------------------------------------------------------------------------

            if n_selected_nodes > 0:  # check that the sub-tree is not empty
                self.sentence, verdict_for_sub_tree, counter_for_sub_tree = self.lexicalize_sub_tree()  # KEY PROCESS
                self.sentence_set.append(self.sentence)  # add created sentence to the sentence set (text).
                self.mark_lexicalized_sub_tree()  # mark the the message tree nodes that have been lexicalized

            if self.count_non_lexicalized_concepts() == 0:  # count if there are non-lexicalized concepts
                lexicalization_complete = True

            self.counter_sub_trees += 1  # the counter of the lexicalization steps

            if self.counter_sub_trees == self.max_number_sub_tree_steps:  # limit num of steps to avoid infinite cycle
                lexicalization_complete = True  # safety precaution


    def select_sub_tree_for_lexicalization(self):
        """
        the message tree is split into sub-trees (message fragments) that are lexicalized separately. each sub-tree
        is converted into a single sentence.
        the selection of the sub-tree works as follows:
        1. the head node of the sub-tree is selected (separate method)
        2. the number of levels below the head node is set randomly (according to probability distribution)
        3. the nodes are considered level by level (for all levels chosen in #2)
        4. the node is considered eligible if it's parent node is selected (or the node is the top message node)
        5. the eligible node is selected according to random choice with parameterized probability of selection.
        """

        def select_head_concept_for_lexicalization():
            """
            choose randomly non-lexicalized concept to become a head of the next lexicalization fragment
            taking into account preferences for level of the concept (the higher the concept level the more
            likely it is to be selected for lexicalization) as well as activation of the concept
            """

            probs = np.zeros(len(self.message))  # initialize array for probabilities of node selection

            if not self.message[0]['is_lexicalized']:  # if the top node is not lexicalized, that node is selected.
                selected_node = 0
            else:  #
                for i, node in enumerate(self.message):  # loop over elements (nodes) of the message
                    parent_node = self.get_parent_node(node)  # parent node
                    if not node['is_lexicalized'] and parent_node['is_lexicalized']:  # if the node is not lexicalized
                        node_level = len(node['code']) - 1  # level of the node, top node has level 0
                        if node_level >= len(self.level_preference_multiplier):  # cap the node_level
                            node_level_ = len(self.level_preference_multiplier) - 1  # if the actual node
                            # level is 3 or higher than it is capped as 2 (in case level_pref_multiplier has 3 items.
                        else:
                            node_level_ = node_level

                        # set the probability of selection according to level preference and salience (activation)
                        probs[i] = self.level_preference_multiplier[node_level_] * 2 ** node['activation']

                if np.sum(probs) != 0:
                    probs /= np.sum(probs)  # normalize probs
                    selected_node = np.random.choice(range(len(self.message)), p=probs)  # select the node to head
                    # lexicalization fragment

                    self.message[selected_node]['is_selected'] = True  # set flag for the head of the current fragment
                    # for lexicalization
                else:
                    selected_node = 0

            return selected_node

        # set the depth of the sub-tree (randomly according to the parameters - probabilities)
        self.sub_tree_depth = np.random.choice(range(len(self.sub_tree_depth_probs)), p=self.sub_tree_depth_probs)

        head_node = select_head_concept_for_lexicalization()  # set the top of the fragment
        starting_level = len(self.message[head_node]['code'])  # 1 if the head node is top

        # define the range of levels of the message tree that can be included in the sub_tree
        levels = range(starting_level, starting_level + self.sub_tree_depth)  # levels include
        # levels from one level below the head node (len(head_node code)) to up to + 2 levels
        # so if the head_node 'x111', its level is 3 and starting level is 4 and levels are
        # 4:4, 4:5, or 4:6 (1,2,3 levels) - good.

        for l in levels:  # levels is a range eg from 3 to 5 (corresponding to x00 to x0000 code formats)
            for i, node in enumerate(self.message):  # loop over all nodes in the message
                node_to_consider = self.message[i]  # take the current node

                # select nodes eligible for lexicalization:
                if len(node_to_consider['code']) == l:  # check if it's of the right level
                    parent_node_to_consider = self.get_parent_node(node_to_consider)  # get parent node
                    if not parent_node_to_consider:  # if there is no parent (beginning of lexicalization)
                        eligibility_flag = True  # if it's a top node, it's eligible
                    else:
                        if parent_node_to_consider['is_selected']:  # check if parent is selected (continuous tree)
                            eligibility_flag = True  # if the parent node is selected it's eligible
                        else:
                            eligibility_flag = False  # not eligible if none of the conditions holds

                    if eligibility_flag:  # if node is indeed eligible
                        # this is where connection shall be added as a parameter of selection
                        # meaning that nodes that have stronger connections shall have probability of
                        # selection. @connections_to_include
                        if np.random.random() < self.sub_tree_growth_rate:  # make a noisy decision based on parameter
                            self.message[i]['is_selected'] = True  # set the flag for selected node (in the message)


    def lexicalize_sub_tree(self):
        """
        3-tier search:
        tier 1: select the head-word
        tier 2: select the way it is expanded (find the expansion root)
        tier 3: select the expansion word (expansion leaf)
        generally speaking for each sub-tree i may try X head-words. an attempt for the headword is a number of
        extensions of the tree. each extension is a choice of root of expansion and a choice of leaf of expansion
        leaf is a node attached to the root.

        :return: best_sentence - the sentence that expresses the sub-tree
        :return: verdict - the method of selecting the sub-tree
        :return: len(tested_head_word) - number of headwords tried to construct the best_sentence
        """

        sub_tree = [n for n in self.message if n['is_selected']]  # sub-tree of the message to be processed
        top_concept = sub_tree[0]  # top concept of the sub-tree. index in the concepts store

        lex_flag = False  # flag to signal the end of lexicalization
        self.tested_head_words = []  # reset memory of tried head words

        while not lex_flag:

            head_word = self.lexicalize(top_concept, self.tested_head_words)  # new head word to try
            self.semantic_strength = self.semantics[head_word, top_concept['value']]  # the distance to the top concept
            self.timing += self.duration_head_word_search  # update time
            self.last_head_word = head_word

            # -------------------------------------------------------------------------------------------------
            # make a log record of the new head word:
            # shape initial 1-word sentence and calculate its distance to the message sub-tree:
            initial_sentence_tree = [{'code': 's', 'value': head_word, 'label': self.lexicon[head_word]}]
            initial_distance = self.evaluate_distance(initial_sentence_tree, sub_tree)

            if initial_distance < self.distance_threshold:
                special_verdict = 'one word sentence accepted'
            else:
                special_verdict = 'expansion is required'
            rec = {'time': float('%.2f' % self.timing),
                   'record_type': 'new_head_word','sub_tree': self.condensed_sub_tree,
                   'threshold': float('%.2f' % self.distance_threshold),
                   'head_word': initial_sentence_tree[0],
                   'semantic_strength': float('%.3f'%self.semantic_strength),
                   'current_sentence': [x['code']+': '+x['label']for x in initial_sentence_tree],
                   'tested_head_words': self.tested_head_words.copy(),
                   'distance': float('%.3f' % initial_distance),
                   'verdict': special_verdict}
            rec_ = {k:self.log_dict[k] if k not in rec else rec[k] for k in self.log_dict}  # to replace nans
            self.log_encoding = self.log_encoding.append(rec_, ignore_index=True)
            # -------------------------------------------------------------------------------------------------

            if initial_distance < self.distance_threshold:
                best_sentence = initial_sentence_tree
                verdict_for_head_word = 'success. one word sentence accepted'
                distance = initial_distance  # for return
                lex_flag = True  # end of lexicalization
            else:  # normal case when one-word sentence does not work

                # call the search for the best sentence built on the head word:
                best_sentence_pack = self.get_best_sentence_for_head_word(sub_tree, head_word)  # KEY PROCESS
                best_sentence, verdict_for_best_sentence, steps_for_best_sentence = best_sentence_pack  # unpack
                distance = self.evaluate_distance(best_sentence, sub_tree)  # check the distance
                self.tested_head_words.append(head_word)  # memorize it has been tried

                if distance < self.distance_threshold:
                    lex_flag = True
                    verdict_for_head_word = 'success'
                elif len(self.tested_head_words) > self.head_word_max_attempts:
                    lex_flag = True
                    verdict_for_head_word = 'no more head words. failure'

        # -------------------------------------------------------------------------------------------------------
        # make a log record:
        rec = {'time': float('%.2f' % self.timing),
               'record_type': 'lexicalization is finished', 'sub_tree': self.condensed_sub_tree,
               'threshold': float('%.2f' % self.distance_threshold),
               'head_word': initial_sentence_tree[0],
               'semantic_strength': float('%.3f' % self.semantic_strength),
               'current_sentence': [x['code'] + ': ' + x['label'] for x in best_sentence],
               'tested_head_words': self.tested_head_words.copy(),
               'distance': float('%.3f' % distance),
               'verdict': verdict_for_head_word}
        rec_ = {k: self.log_dict[k] if k not in rec else rec[k] for k in self.log_dict}  # to replace nans
        self.log_encoding = self.log_encoding.append(rec_, ignore_index=True)
        # -------------------------------------------------------------------------------------------------------

        return best_sentence, verdict_for_head_word, len(self.tested_head_words)


    def get_best_sentence_for_head_word(self, sub_tree, head_word):
        """
        builds the best possible sentence given a message sub_tree and a head_word
        :param sub_tree:
        :param head_word
        :return:
        """

        # form the initial tree:
        current_sentence_tree = [{'code': 's', 'value': head_word, 'label': self.lexicon[head_word]}]
        current_distance = self.evaluate_distance(current_sentence_tree, sub_tree)
        flag = False
        counter = 0
        self.excluded_root_nodes = []  # reset the memory for tested expansion roots

        while not flag:

            # choose the root for the tree expansion (the value is the index in the lexical tree):
            expansion_root_node = self.get_node_for_expansion(current_sentence_tree)
            self.timing += self.duration_root_search  # update time

            if expansion_root_node != 'None':
                self.excluded_root_nodes.append(expansion_root_node)
                self.expansion_root_code = current_sentence_tree[expansion_root_node]['code']
                self.expansion_root_label = current_sentence_tree[expansion_root_node]['label']
                special_verdict = 'start search with new expansion root'
            else:
                self.expansion_root_code = 'None'
                self.expansion_root_label = 'None'
                special_verdict = 'no expansion root found. try another head-word'

            # -------------------------------------------------------------------------------------------------
            # make a log record of the new expansion root:
            rec = {'time': float('%.2f' % self.timing),
                   'record_type': 'new_expansion_root',
                   'sub_tree': self.condensed_sub_tree,
                   'threshold': float('%.2f' % self.distance_threshold),
                   'head_word': current_sentence_tree[0],
                   'semantic_strength': float('%.3f' % self.semantic_strength),
                   'current_sentence': [x['code']+': '+x['label'] for x in current_sentence_tree],
                   'tested_head_words': self.tested_head_words.copy(),
                   'root_code': self.expansion_root_code,
                   'root_counter': counter,  # number of roots tested
                   'excluded_root_codes': [current_sentence_tree[x]['code'] for x in self.excluded_root_nodes],
                   'root_word': self.expansion_root_label,
                   'distance': float('%.3f' % current_distance),
                   'verdict': special_verdict}
            rec_ = {k: self.log_dict[k] if k not in rec else rec[k] for k in self.log_dict}
            self.log_encoding = self.log_encoding.append(rec_, ignore_index=True)
            # -------------------------------------------------------------------------------------------------

            if expansion_root_node == 'None':  # no expansion root is found
                flag = True
                best_expansion_root_tree = current_sentence_tree  # the last tested tree is returned.
                verdict_for_best_sentence = 'all expansion roots have been tried'
            else:
                # call the leaf search / expansion tree building method:
                expansion_return = self.get_best_expansion_leaf_tree(sub_tree,
                                                                     current_sentence_tree,
                                                                     expansion_root_node)

                best_expansion_root_tree, verdict_for_leaf_search, counter_for_leaf_search = expansion_return  # unpack
                best_expansion_root_distance = self.evaluate_distance(best_expansion_root_tree, sub_tree)

                if best_expansion_root_distance < self.distance_threshold:
                    flag = True
                    verdict_for_best_sentence = 'success'
                elif current_distance - best_expansion_root_distance > self.increment_threshold:  # improves distance
                    # recursion. try to expand the new expanded tree.
                    current_sentence_tree = best_expansion_root_tree  # new current tree.
                    current_distance = best_expansion_root_distance  # !!!! update the current distance
                    self.excluded_root_nodes = []  # reset excluded roots for expansion
                    verdict_for_best_sentence = 'incremental improvement'
                elif counter > self.expansion_root_node_max_attempts:
                    flag = True
                    verdict_for_best_sentence = 'failure'
                else:
                    verdict_for_best_sentence = 'try another expansion'
                counter += 1

        return best_expansion_root_tree, verdict_for_best_sentence, counter


    def get_node_for_expansion(self, lexical_tree):
        """
        the method chooses one of the nodes of the lexical tree as designated for expansion.
        the probability of the node to be chosen depends on:
        - level of the node in the tree hierarchy (higher level are supposed to be chosen preferably)
        - number of children current node already has (the more children is has the less likely to be chosen)
        :param lexical_tree:
        :return: selected_node_index (index of the selected node in the tree)
        """

        max_levels = len(self.sentence_tree_shape)  # maximum number of recursions in the lexical tree
        probs = []  # initialize probabilities

        for i, node in enumerate(lexical_tree):  # calculate multipliers for probs for each node

            n_children = self.get_number_of_children(lexical_tree, i)  # get the number of existing children.
            node_level = len(node['code']) - 1  # calculate the level of the given node. head is level 0

            if node_level > max_levels - 1:  # limit of recursion is reached
                node_expansion_multiplier = 0  # no further expansion
            elif n_children > self.sentence_tree_shape[node_level]:  # if limit of number of children is reached
                node_expansion_multiplier = 0
            elif i in self.excluded_root_nodes:  # if the node has been tried already
                node_expansion_multiplier = 0
            else:  # cap node_levels to reflect expansion preferences
                if node_level >= len(self.level_expansion_multiplier):  # clipping the node_level
                    node_level_ = len(self.level_expansion_multiplier) - 1  # max node_level_ is 2
                else:
                    node_level_ = node_level

                if n_children >= len(self.n_children_expansion_multiplier):  # clipping the number of children
                    n_children_ = len(self.n_children_expansion_multiplier) - 1  # max n_children = 2
                else:
                    n_children_ = n_children

                a = self.level_expansion_multiplier[node_level_]
                b = self.n_children_expansion_multiplier[n_children_]
                node_expansion_multiplier = a * b

            probs.append(node_expansion_multiplier)

        probs = np.array(probs)  # convert to numpy array
        probs_sum = np.sum(probs)  # sum of probs for normalization



        if probs_sum > 0:
            probs = probs/probs_sum  # normalize probabilities
            selected_node_index = np.random.choice(range(len(probs)), p=probs)
        else:
            selected_node_index = 'None'  # no expansion is possible as all nodes have maximum number of children

        return selected_node_index  # returns index in the lexical tree. value 'None' is a signal no legit expansion


    def get_best_expansion_leaf_tree(self, sub_tree, current_tree, expansion_root_node):
        """
        :param current_tree:
        :param sub_tree:
        :param expansion_root_node: is the index of the node in the tree to expand (that's cool)
        :return: candidate_expansion_tree: the current tree with added expansion leaf.
        :return: verdict: indicator of how the decision was made
        :return: counter: the number of words tried before the verdict was reached
        """

        expansion_root_code = current_tree[expansion_root_node]['code']  # the code of the expansion root node
        expansion_root_word = current_tree[expansion_root_node]['value']  # this is the index of the word from
        # the lexicon corresponding to the selected expansion node.

        excluded_words = []
        flag = False
        counter = 0

        while not flag:  # loop over potential leafs to expand the lexical tree

            excluded_words = [n['label'] for n in current_tree]  # avoid repetition
            expansion_leaf_word = self.collocate(expansion_root_word, excluded_words)  # word index!
            excluded_words.append(expansion_leaf_word)  # store the tried leaf to exclude from later iterations
            self.timing += self.duration_leaf_search  # update time

            expansion_leaf_node_full = {'code': self.get_next_code(expansion_root_code, current_tree),
                                        'value': expansion_leaf_word,
                                        'label': self.lexicon[expansion_leaf_word]}

            # construct the candidate expansion tree:
            candidate_expansion_tree = current_tree.copy()
            candidate_expansion_tree.append(expansion_leaf_node_full)  # this is the candidate lexical tree

            # get the distances:
            candidate_distance = self.evaluate_distance(candidate_expansion_tree, sub_tree)
            current_distance = self.evaluate_distance(current_tree, sub_tree)

            if candidate_distance < self.distance_threshold:
                flag = True
                verdict_for_leaf = 'success. sentence accepted'
            elif current_distance - candidate_distance > self.increment_threshold:
                flag = True
                verdict_for_leaf = 'incremental_success. word accepted'
            elif counter > self.expansion_leaf_node_max_attempts:  # if maximum number of attempts is reached
                flag = True
                verdict_for_leaf = 'timeout: max number of leafs'
            else:
                verdict_for_leaf = 'continue searching leafs'

            syntactic_strength = self.collocations[expansion_root_word,expansion_leaf_word]

            # -------------------------------------------------------------------------------------------------
            # make a log record of the leaf tested:
            rec = {'time': float('%.2f' % self.timing),
                   'record_type': 'new_leaf',
                   'sub_tree': self.condensed_sub_tree,
                   'threshold': float('%.2f' % self.distance_threshold),
                   'head_word': current_tree[0],
                   'semantic_strength': float('%.3f' % self.semantic_strength),
                   'tested_head_words': self.tested_head_words.copy(),
                   'current_sentence': [x['code']+': '+x['label'] for x in current_tree],
                   'candidate_sentence': [x['code']+': '+x['label'] for x in candidate_expansion_tree],
                   'root_code': self.expansion_root_code,
                   'excluded_root_codes': [current_tree[x]['code'] for x in self.excluded_root_nodes],
                   'root_word': self.expansion_root_label,
                   'root_counter': '',  # to remove annoying 'nan' from the dataframe view
                   'leaf_word': self.lexicon[expansion_leaf_word],
                   'leaf_counter': counter,  # number of leafs tested
                   'syntactic_strength': float('%.3f' % syntactic_strength),
                   'verdict': verdict_for_leaf,
                   'distance': float('%.3f' % current_distance),
                   'candidate_distance': float('%.3f' % candidate_distance)}
            rec_ = {k: self.log_dict[k] if k not in rec else rec[k] for k in self.log_dict}  # to replace nans
            self.log_encoding = self.log_encoding.append(rec_, ignore_index=True)
            # -------------------------------------------------------------------------------------------------

            counter += 1

        if verdict_for_leaf == 'timeout: max number of leafs':
            candidate_expansion_tree = current_tree.copy()  # return original tree.

        return candidate_expansion_tree, verdict_for_leaf, counter


    def lexicalize(self, concept, excluded_words=[]):
        """
        find a word that has not been tried yet with some randomization
        :param concept: concept node (element of the message) that needs to be lexicalized
        :param excluded_words: the words that shall be excluded from the search (indices from lexicon)
        :return: selected_word - an index of the element of the lexicon
        """
        noise_indicator = self.lexicalization_noise_level

        selection = self.semantics[:, concept['value']]  # get the column of the semantic matrix
        # corresponding to a given concept.

        # shut down activation of the excluded words:
        selection_filtered = [a if i not in excluded_words else 0 for i, a in enumerate(selection)]

        if selection_filtered:  # if not all words are excluded
            noise_component = np.random.normal(0, noise_indicator, len(selection))  # random activation
            selection_randomized = selection_filtered + noise_component  # add noise
            selected_word = np.argmax(selection_randomized)  # choose the word
        else:
            selected_word = 'None'  # signal that all words have been tested.

        return selected_word


    def collocate(self, word, excluded_words=[]):
        """
        find a word that collocates with the current word with possible exclusion of some of the words
        :param word: index from the lexicon representing the word to find the collocation for.
        :param excluded_words: words that are excludes as potential collocations.
        :return: selected word (index in the lexicon)
        """
        noise_indicator = self.collocations_noise_level
        selection = self.collocations[word]  # select the row of the syntactic association matrix (collocations)

        # shut down activation of the excluded words:
        selection_filtered = [a if i not in excluded_words else 0 for i, a in enumerate(selection)]

        if selection_filtered:
            noise_component = np.random.normal(0, noise_indicator, len(selection_filtered))  # noise activation
            selection_randomized = selection_filtered + noise_component  # total activation including noise
            selected_word = np.argmax(selection_randomized)  # find the word with the highest activation
        else:
            selected_word = 'None'  # signal that all words have been tested and no further selection is possible[

        return selected_word


    def evaluate_distance(self, lexical_tree, concept_tree):
        """
        the function measures the distance (=quality of expression) between the conceptual tree (message) and
        lexical tree (expression, phrase). the distance is a weighted sum of squares of the distances
        between each word and each concept scaled by the level of word and concept.
        the idea behind the weighting is that the concepts higher in the hierarchy are more important
        and shall be preferentially expressed. the weighting of words is based on the idea that words higher
        in the hierarchy are more salient and have greater influence on comprehension than lower hierarchy
        words.
        :param lexical_tree: list of tuples of the shape (node_code, node_value, node_activation)
        :param concept_tree: the same as lexical tree.
        :return: the real number representing the distance between two trees. the smaller the distance, the
        better is the quality of expression. the distance is NOT normalized per word or per concept, so
        it depends on the size of the tree.
        the good words have a greater distance
        """

        distance = 0
        for concept in concept_tree:
            distance_to_concept = 1
            concept_level = len(concept['code']) - 1
            concept_level = min(concept_level, len(self.concept_level_weights) - 1)  # capping the concept level
            concept_weight = self.concept_level_weights[concept_level]  # importance of the concept.
            for word in lexical_tree:
                word_level = len(word['code']) - 1
                word_level = min(word_level, len(self.lexical_tree_weights) - 1)  # cap word level
                word_weight = self.lexical_tree_weights[word_level]
                sem_association = self.semantics[word['value'], concept['value']]
                distance_to_concept *= (1 - sem_association) * concept_weight * word_weight
            distance += distance_to_concept
        return distance


    # =============================================================================================================
    # helper/convenience/interface methods:
    # =============================================================================================================


    def mark_lexicalized_sub_tree(self):  # mark lexicalized sub tree accordingly and unmark selection
        for node in self.message:
            if node['is_selected']:
                node['is_lexicalized'] = True
                node['is_selected'] = False


    def count_non_lexicalized_concepts(self):  # count the number of concepts not yet lexicalized
        counter = 0
        for c in self.message:
            if not c['is_lexicalized']:
                counter += 1
        return counter


    def get_parent_node(self, node, generations=1):  # convenience function to return a parent node of a give node
        try:
            parent_code = node['code'][:-generations]
            parent_node = [n for n in self.message if n['code'] == parent_code][0]
            return parent_node
        except IndexError:
            return []


    @staticmethod
    def get_number_of_children(tree, node):  # checks the node codes and return the number of children
        # node is supplied as index of the node in the tree list.
        node_code = tree[node]['code']  # get the code of the node
        n_children = 0
        for n in tree:  # loop over the nodes of the tree
            if n['code'][:-1] == node_code:
                n_children += 1
        return n_children


    @staticmethod
    def get_next_code(node_code, current_tree):  # get the new code. convenience function
        """
        generates the new code which will be the sibling of a parameter node.
        e.g. if node_code = 's101' and there are 's100' and 's102' in the sentence
        the function returns 's103' and the code to be used for the nex word
        :param node_code:
        :param current_tree:
        :return: new code
        """
        n_children = 0
        for n in current_tree:  # loop over the words in the sentence
            if n['code'][:-1] == node_code:  # look for the siblings of the current node.
                n_children += 1  # count the siblings
        return node_code + str(n_children)  # return the next sibling


    def reset_message(self):  # reset the message for another lexicalization attempt
        for n in self.message:
            n['is_lexicalized'] = False
            n['is_selected'] = False


    def draw_tree(self, x):
        """
        this is a method for drawing tree from any list of dictionaries that has hierarchical 'code' and 'label'
        keys
        """
        self.tree = list()
        tree_node = x[0]['code'] + ': ' + str(x[0]['label'])
        self.tree.append(Node(tree_node))
        code_list = [m['code'] for m in x]

        for nd in x[1:]:
            parent_index = code_list.index(nd['code'][:-1])  # index of the parent of the current node
            tree_node = nd['code'] + ': ' + str(nd['label'])
            self.tree.append(Node(tree_node, parent=self.tree[parent_index]))

        for pre, fill, node in RenderTree(self.tree[0]):
            print('%s%s' % (pre, node.name))


    def draw_message(self):  # convert message into multi-tiered list
        """
        method draws the tree representing the message
        """
        self.tree = list()
        tree_node = self.message[0]['code'] + ': ' + str(self.message[0]['label'])
        self.tree.append(Node(tree_node))
        code_list = [m['code'] for m in self.message]

        for nd in self.message[1:]:
            parent_index = code_list.index(nd['code'][:-1])  # index of the parent of the current node
            tree_node = nd['code'] + ': ' + str(nd['label'])
            self.tree.append(Node(tree_node, parent=self.tree[parent_index]))

        for pre, fill, node in RenderTree(self.tree[0]):
            print('%s%s' % (pre, node.name))


    def print_sentence_set(self):  # formatted printing of the generated sentences imitating natural language
        sentence_set_text = ''
        for s in self.sentence_set:  # loop over sentences in the sentence set
            sentence_text = ''
            for i, w in enumerate(s):  # loop over words in the single sentence
                word_text = str(w['label'])  # the content of the current word
                if i != 0 and i < len(s):  # excluding the first and the last word
                    if len(w['code']) == len(s[i-1]['code']):  # if the word is a sibling of the previous word
                        word_text = ' and ' + word_text  # connect them with and
                    else:  # if the word is either a parent or a child connect them with that or which
                        if len(w['code']) % 2 == 0:  # alternate between that and which
                            word_text = ' that ' + word_text
                        else:
                            word_text = ' which ' + word_text
                sentence_text += word_text  # add word to the sentence
            sentence_text = sentence_text[0].upper() + sentence_text[1:] + '. '  # capital letter and full stop
            sentence_set_text += sentence_text
        return sentence_set_text[:-1]



